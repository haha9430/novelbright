import json
import time
import re
from typing import List, Dict, Any, Set
import difflib

# LangChain & AI ê´€ë ¨
from langchain_upstage import ChatUpstage
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.utilities import GoogleSerperAPIWrapper
from langchain_community.tools.tavily_search import TavilySearchResults

# ë¡œì»¬ DB ë ˆí¬ì§€í† ë¦¬
from app.service.clio_fact_checker_agent.repo import ManuscriptRepository

class ManuscriptAnalyzer:
    def __init__(self, setting_path: str, character_path: str): # [ë³€ê²½] character_path ì¶”ê°€
        # 1. LLM ì„¤ì •
        self.llm = ChatUpstage(model="solar-pro")

        # 2. ì„¤ì • íŒŒì¼ ë¡œë“œ
        # plot.json (ê¸°ì¡´)
        self.settings = self._load_settings(setting_path)
        # characters.json (ì‹ ê·œ ì¶”ê°€) -> ì—¬ê¸°ì„œ ë¡œë“œí•©ë‹ˆë‹¤.
        self.character_data = self._load_settings(character_path)

        # 3. í—ˆêµ¬/ì„¤ì • í‚¤ì›Œë“œ ì¶”ì¶œ (ë‘ íŒŒì¼ ë‚´ìš©ì„ í•©ì³ì„œ í•„í„°ë§ ëª©ë¡ ìƒì„±)
        self.setting_keywords = self._extract_setting_keywords()

        # 4. ë¦¬í¬ì§€í† ë¦¬ ë° íˆ´ ì´ˆê¸°í™” (ê¸°ì¡´ ë™ì¼)
        self.repo = ManuscriptRepository()
        self.search_tool = TavilySearchResults(k=5, search_depth="advanced")
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", ". ", " ", ""]
        )

    def _load_settings(self, path: str) -> Dict[str, Any]:
        """ì„¤ì • íŒŒì¼(JSON)ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"âš ï¸ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")
            return {}

    def _extract_setting_keywords(self) -> Set[str]:
        """ì†Œì„¤ ì† í—ˆêµ¬ì˜ ê³ ìœ ëª…ì‚¬ + characters.jsonì˜ ì¸ë¬¼ë“¤ì„ í•„í„°ë§ í‚¤ì›Œë“œë¡œ ì¶”ì¶œ"""
        keywords = set()

        # 1. plot.json ë°ì´í„° ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        plot_data = self.settings
        for char in plot_data.get("characters", []):
            name = char.get("name", "").strip()
            if name: keywords.add(name)

        factions = plot_data.get("world_view", {}).get("factions", [])
        for f in factions:
            if isinstance(f, str):
                keywords.add(f.split("(")[0].strip())

        # 2. [ì¶”ê°€] characters.json ë°ì´í„° ì²˜ë¦¬
        # ì œê³µí•´ì£¼ì‹  ì–‘ì‹ì€ {"ì´ë¦„": {ìƒì„¸ì •ë³´}, ...} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ì…ë‹ˆë‹¤.
        if self.character_data:
            for name_key in self.character_data.keys():
                # "ê¹€íƒœí‰", "ì´ë„í›ˆ", "ë”ê¸€ëŸ¬ìŠ¤ í—¤ì´ê·¸" ë“±ì˜ í‚¤ê°’ì„ ì¶”ê°€
                keywords.add(name_key.strip())

        return keywords

    def analyze_manuscript(self, text: str) -> Dict[str, Any]:
        """
        [ìµœì¢… ìˆ˜ì •] 1ì°¨(íƒì§€) + 2ì°¨(ê°ìˆ˜) ì˜ê²¬ ë™ì‹œ ë¦¬í¬íŒ… ë²„ì „
        """
        print(f"ğŸ“„ ì •ë°€ ê³ ì¦ ë¶„ì„ ì‹œì‘ (ì´ {len(text)}ì)")

        # 1. í…ìŠ¤íŠ¸ ë¶„í•  ë° ëª…ì œ(Query) ì¶”ì¶œ
        chunks = self.text_splitter.split_text(text)
        all_query_items = []

        for i, chunk in enumerate(chunks):
            # [í† í° ì••ì¶•] ë‚´ë¶€ì—ì„œ _compress_text í˜¸ì¶œ
            items = self._extract_search_queries(chunk)

            for item in items:
                kw = item['keyword']
                origin_snippet = item.get('original_sentence', '')

                # --- ìœ„ì¹˜ ì°¾ê¸° ë¡œì§ ---
                start_idx, end_idx = self._find_exact_position(text, origin_snippet, 0)

                # ë‚´ìš© ì¼ì¹˜ í™•ì¸
                is_match = False
                if start_idx != -1:
                    actual_text = text[start_idx:end_idx]
                    if re.sub(r'[\s\W_]+', '', actual_text) == re.sub(r'[\s\W_]+', '', origin_snippet):
                        is_match = True

                # ì¬ì‹œë„
                if start_idx == -1 or not is_match:
                    new_snippet = self._retry_extract_sentence(chunk, kw)
                    if new_snippet:
                        s_idx, e_idx = self._find_exact_position(text, new_snippet, 0)
                        if s_idx != -1:
                            item['original_sentence'] = new_snippet
                            start_idx, end_idx = s_idx, e_idx

                item['start_index'] = start_idx
                item['end_index'] = end_idx
                all_query_items.append(item)

        print(f"   -> ì´ {len(all_query_items)}ê°œì˜ ê²€ì¦ ëª…ì œ ì¶”ì¶œë¨")

        known_settings = []
        historical_context = []
        verification_queue = []

        # 2. ê²€ìƒ‰ ìˆ˜í–‰
        for item_data in all_query_items:
            proposition = item_data['keyword']
            query_string = item_data['search_query']
            origin_sent = item_data.get('original_sentence', '')

            # í—ˆêµ¬ í•„í„°ë§
            is_fiction = False
            for fiction_term in self.setting_keywords:
                if fiction_term in proposition or fiction_term in origin_sent:
                    is_fiction = True
                    break

            if is_fiction:
                known_settings.append(proposition)
                continue

            print(f"ğŸ” ê²€ìƒ‰ ìˆ˜í–‰: '{query_string}'")

            search_data = self._check_local_db(query_string)
            if not search_data:
                search_data = self._search_web(query_string)
                time.sleep(0.1)

            if search_data:
                verification_queue.append({
                    "id": len(verification_queue),
                    "keyword": proposition,
                    "query": query_string,
                    "content": search_data['content'],
                    "context": origin_sent,
                    "item_data": item_data,
                    "search_source": search_data.get('source', 'Unknown')
                })

        # 3. ì¼ê´„/êµì°¨ ê²€ì¦ (Batch Verification)
        if verification_queue:
            print(f"ğŸš€ ì´ {len(verification_queue)}ê±´ì— ëŒ€í•´ íŒ©íŠ¸ì²´í¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤...")

            BATCH_SIZE = 5
            for i in range(0, len(verification_queue), BATCH_SIZE):
                batch_items = verification_queue[i : i + BATCH_SIZE]
                print(f"   -> Batch {i//BATCH_SIZE + 1} ì²˜ë¦¬ ì¤‘...")

                # 1ì°¨ & 2ì°¨ ê²€ì¦ ìˆ˜í–‰
                first_results = self._verify_batch_relevance(batch_items)
                final_results = self._double_check_batch_results(batch_items, first_results)

                # ê²°ê³¼ ë§¤í•‘
                for item in batch_items:
                    item_id = str(item['id'])

                    # 1ì°¨, 2ì°¨ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ ë¹ˆ ë”•ì…”ë„ˆë¦¬)
                    res_1 = first_results.get(item_id, {})
                    res_2 = final_results.get(item_id, {})

                    # ìµœì¢… íŒì • ì—¬ë¶€ (2ì°¨ ê²°ê³¼ ê¸°ì¤€, ì—†ìœ¼ë©´ 1ì°¨ ê¸°ì¤€, ë‘˜ ë‹¤ ì—†ìœ¼ë©´ í†µê³¼ë¡œ ê°„ì£¼)
                    # [ë³€ê²½] ëª¨ë“  ê²°ê³¼ë¥¼ ë‹´ê¸° ìœ„í•´ ë³€ìˆ˜ì— ì €ì¥
                    final_is_positive = res_2.get('is_positive', res_1.get('is_positive', True))

                    # ê·¼ê±° í•©ì¹˜ê¸°
                    reason_1 = res_1.get('reason', 'ê·¼ê±° ì—†ìŒ')
                    reason_2 = res_2.get('reason', 'ì¶”ê°€ ì˜ê²¬ ì—†ìŒ')
                    combined_reason = f"[1ì°¨ íƒì§€] {reason_1}\n[2ì°¨ ê°ìˆ˜] {reason_2}"

                    # ê²°ê³¼ ê°ì²´ ìƒì„±
                    final_obj = {
                        "keyword": item['keyword'],
                        "is_positive": final_is_positive,  # âœ… í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì´ˆë¡/ë¹¨ê°• ë°•ìŠ¤ êµ¬ë¶„ìš©
                        "reason": combined_reason,
                        "original_sentence": item['context'],
                        "source": item['search_source'],
                        "start_index": item['item_data'].get('start_index'),
                        "end_index": item['item_data'].get('end_index')
                    }

                    # ğŸš¨ [í•µì‹¬ ìˆ˜ì •] ì¡°ê±´ë¬¸ ì—†ì´ ë¬´ì¡°ê±´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ (ëª¨ë“  ê²°ê³¼ í‘œì‹œ)
                    historical_context.append(final_obj)

                    # ì½˜ì†” ë¡œê·¸ì—ëŠ” ë³´ê¸° ì¢‹ê²Œ êµ¬ë¶„í•´ì„œ ì¶œë ¥
                    if final_is_positive:
                        print(f"      âœ… [í†µê³¼] {item['keyword']}")
                    else:
                        print(f"      âŒ [ì˜¤ë¥˜] {item['keyword']}")

        # ìµœì¢… ë°˜í™˜ê°’ êµ¬ì„±
        return {
            "total_checked": len(all_query_items),
            # ì˜¤ë¥˜ ê°œìˆ˜ëŠ” ë¦¬ìŠ¤íŠ¸ì—ì„œ 'is_positive'ê°€ Falseì¸ ê²ƒë§Œ ì„¸ì–´ì„œ ë³„ë„ ì œê³µ
            "error_count": len([i for i in historical_context if not i['is_positive']]),
            "historical_context": historical_context, # ì—¬ê¸°ì— í†µê³¼+ì˜¤ë¥˜ í•­ëª©ì´ ëª¨ë‘ ë“¤ì–´ê°‘ë‹ˆë‹¤.
            "setting_terms_found": list(set(known_settings))
        }

    def _extract_search_queries(self, text: str) -> List[Dict[str, str]]:
        # [ìˆ˜ì •ë¨] í† í° ì••ì¶•(_compress_text) ê³¼ì •ì„ ì œê±°í•˜ê³  ì›ë¬¸ì„ ë°”ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.

        # compressed_text = self._compress_text(text)  <-- ì´ ë¶€ë¶„ì„ ì‚­ì œí•˜ê±°ë‚˜ ì£¼ì„ ì²˜ë¦¬

        # ë¡œê·¸ (ì„ íƒ ì‚¬í•­)
        # print(f"ğŸ“„ ëª…ì œ ì¶”ì¶œ ìš”ì²­ (ê¸¸ì´: {len(text)})")

        prompt = """
        ë‹¹ì‹ ì€ ì—­ì‚¬ ì†Œì„¤ì˜ 'ë¯¸ì„¸ ê³ ì¦ ê°ë³„ì‚¬'ì…ë‹ˆë‹¤.
        ì…ë ¥ëœ 'ìš”ì•½ í…ìŠ¤íŠ¸'ë¥¼ ë³´ê³  ì—­ì‚¬ì  ì‚¬ì‹¤ í™•ì¸ì´ í•„ìš”í•œ **'ê²€ì¦ ëª…ì œ'**ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

        [ì¶”ì¶œ ê°€ì´ë“œë¼ì¸]
        1. ë‹¨ìˆœ ë‹¨ì–´(ì˜ˆ: 'ì´')ê°€ ì•„ë‹ˆë¼, **"ëˆ„ê°€/ì–¸ì œ/ì–´ë””ì„œ/ë¬´ì—‡ì„ í–ˆëŠ”ê°€"**ê°€ í¬í•¨ëœ êµ¬ì²´ì  ëª…ì œë¡œ ë§Œë“œì„¸ìš”.
           - Bad: "ê²Œë² ì–´ 98"
           - Good: "1916ë…„ ë…ì¼êµ°ì´ ê²Œë² ì–´ 98ì„ ì£¼ë ¥ìœ¼ë¡œ ì‚¬ìš©í–ˆëŠ”ì§€ ì—¬ë¶€"
        2. í•œ ë¬¸ì¥ì— ì—¬ëŸ¬ ê²€ì¦ í¬ì¸íŠ¸ê°€ ìˆë‹¤ë©´ ìª¼ê°œì„œ ì¶”ì¶œí•˜ì„¸ìš”.
           - ì˜ˆ: "ì´ë„í›ˆì€ ì°¸í˜¸ì—ì„œ MREë¥¼ ë¨¹ì—ˆë‹¤." 
             -> ëª…ì œ 1: "1ì°¨ ëŒ€ì „ ì°¸í˜¸ì „ì˜ êµ¬ì¡° ë° í™˜ê²½"
             -> ëª…ì œ 2: "1ì°¨ ëŒ€ì „ ë‹¹ì‹œ MRE(ì „íˆ¬ì‹ëŸ‰) ì¡´ì¬ ì—¬ë¶€"

        [ì¶œë ¥ í˜•ì‹]
        ë°˜ë“œì‹œ ì•„ë˜ì™€ ê°™ì€ JSON ë¦¬ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
        [
            {
                "keyword": "ê²€ì¦í•  ëª…ì œ ë‚´ìš©",
                "original_sentence": "ë³¸ë¬¸ì—ì„œ ë°œì·Œí•œ ì›ë¬¸ ë¬¸ì¥",
                "search_query": "Tavily ê²€ìƒ‰ì„ ìœ„í•œ ìµœì í™”ëœ ì˜ì–´/í•œê¸€ ì¿¼ë¦¬",
                "reason": "ê²€ì¦ ì´ìœ "
            }
        ]
        """
        try:
            response = self.llm.invoke([
                SystemMessage(content=prompt),
                # [ì¤‘ìš”] ì••ì¶•ëœ í…ìŠ¤íŠ¸ ëŒ€ì‹  ì›ë³¸ 'text'ë¥¼ ê·¸ëŒ€ë¡œ ë„£ìŠµë‹ˆë‹¤.
                HumanMessage(content=f"Text: {text[:4000]}")
            ])
            return self._parse_json_garbage(response.content)
        except Exception as e:
            print(f"âš ï¸ ëª…ì œ ì¶”ì¶œ ì—ëŸ¬: {e}")
            return []

    def _check_local_db(self, keyword: str) -> Dict[str, Any]:
        """ë¡œì»¬ ë²¡í„° DB ì¡°íšŒ"""
        try:
            # ê²€ìƒ‰
            search_result = self.repo.search(query_text=keyword, n_results=1)

            if not search_result['documents'][0]:
                return None

            dist = search_result['distances'][0][0]
            content = search_result['documents'][0][0]

            # ê±°ë¦¬ ì„ê³„ê°’ (1.0ë³´ë‹¤ ê°€ê¹Œì›Œì•¼ ê´€ë ¨ì„± ìˆìŒ)
            if dist < 1.0:
                return {
                    "keyword": keyword,
                    "content": content,
                    "source": "Local History DB",
                    "confidence": round(1 - (dist/2), 2)
                }
            return None
        except Exception:
            return None

    def _search_web(self, query: str) -> Dict[str, Any]:
        """Tavily AI ì›¹ ê²€ìƒ‰"""
        try:
            # ê²€ìƒ‰ì–´ ë³´ì • (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
            if "ì—­ì‚¬" not in query and "history" not in query.lower():
                final_query = f"{query} ì—­ì‚¬ history"
            else:
                final_query = query

            # Tavily ê²€ìƒ‰ ì‹¤í–‰ (ê²°ê³¼ëŠ” ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë°˜í™˜ë¨)
            # [{'url': '...', 'content': '...'}, ...]
            search_results = self.search_tool.run(final_query)

            if not search_results:
                return None

            # ì—¬ëŸ¬ ê°œì˜ ê²€ìƒ‰ ê²°ê³¼ ë³¸ë¬¸ì„ í•˜ë‚˜ë¡œ í•©ì¹¨
            combined_content = "\n\n".join([
                f"[Source: {res['url']}]\n{res['content']}"
                for res in search_results
            ])

            return {
                "keyword": query,
                "content": combined_content,
                "source": "Web Search (Tavily AI)"
            }
        except Exception as e:
            print(f"âš ï¸ Tavily ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None

    def _verify_batch_relevance(self, batch_items: List[Dict]) -> Dict[str, Dict]:
        """[1ì°¨ ê²€ì¦] ID ê¸°ë°˜ ê²°ê³¼ ë§¤í•‘"""
        items_text = ""
        for item in batch_items:
            items_text += f"""
            ---
            [ID: {item['id']}]
            - ê²€ì¦ ëª…ì œ: {item['keyword']}
            - ì†Œì„¤ ë§¥ë½: {item['context']}
            - ê²€ìƒ‰ ê²°ê³¼: {item['content'][:800]}
            """

        prompt = f"""
        ì—­ì‚¬ íŒ©íŠ¸ì²´ì»¤ì…ë‹ˆë‹¤. ì•„ë˜ í•­ëª©ë“¤ì˜ ì‚¬ì‹¤ ì—¬ë¶€ë¥¼ ê²€ì¦í•˜ì„¸ìš”.

        [íŒë‹¨ ê¸°ì¤€]
        - **is_positive**: ëª…ì œê°€ ì—­ì‚¬ì  ì‚¬ì‹¤ê³¼ ë¶€í•©í•˜ë©´ true, **ì˜¤ë¥˜ë‚˜ ì‹œëŒ€ì°©ì˜¤ë©´ false**.
        - **is_relevant**: ê²€ìƒ‰ ìë£Œê°€ ìœ íš¨í•˜ë©´ true.

        [ì¶œë ¥ í˜•ì‹]
        ë°˜ë“œì‹œ í•­ëª©ì˜ **ID(ìˆ«ì ë¬¸ìì—´)**ë¥¼ í‚¤(Key)ë¡œ í•˜ëŠ” JSONì„ ë°˜í™˜í•˜ì„¸ìš”.
        {{
            "0": {{ "is_relevant": true, "is_positive": false, "reason": "1916ë…„ì—ëŠ” MREê°€ ì—†ì—ˆìŒ" }},
            "1": {{ ... }}
        }}
        """
        try:
            response = self.llm.invoke([SystemMessage(content=prompt)])
            return self._clean_json_string(response.content)
        except Exception: return {}

    def _parse_json_garbage(self, text: str) -> List[Dict]:
        """LLMì´ ì£¼ëŠ” ì§€ì €ë¶„í•œ JSON ë¬¸ìì—´ì—ì„œ ë¦¬ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ"""
        try:
            # ë§ˆí¬ë‹¤ìš´ ì œê±°
            text = text.replace("```json", "").replace("```", "").strip()

            # ê°€ì¥ ë°”ê¹¥ìª½ ëŒ€ê´„í˜¸ ì°¾ê¸°
            start = text.find('[')
            end = text.rfind(']')
            if start != -1 and end != -1:
                json_str = text[start:end+1]
                return json.loads(json_str)
            return []
        except:
            return []

    def _clean_json_string(self, text: str) -> str:
        """
                [ìˆ˜ì •ë¨] ì…ë ¥ì´ ì´ë¯¸ Dictë¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ê³ ,
                Stringì´ë¼ë©´ JSON êµ¬ê°„ë§Œ ì¶”ì¶œí•˜ì—¬ íŒŒì‹±í•©ë‹ˆë‹¤.
                (TypeError ë°©ì§€ìš© ë°©ì–´ ì½”ë“œ í¬í•¨)
                """
        # 1. ì…ë ¥ì´ ì´ë¯¸ ë”•ì…”ë„ˆë¦¬(Dict)ë¼ë©´ íŒŒì‹±í•  í•„ìš” ì—†ì´ ë°”ë¡œ ë°˜í™˜
        if isinstance(text, dict):
            return text

        # 2. ë¬¸ìì—´ì´ ì•„ë‹ˆë¼ë©´(None ë“±) ë¹ˆ Dict ë°˜í™˜
        if not isinstance(text, str):
            return {}

        try:
            # 3. ë§ˆí¬ë‹¤ìš´ ë° ê³µë°± ì œê±°
            text = text.replace("```json", "").replace("```", "").strip()

            # 4. ê°€ì¥ ë°”ê¹¥ìª½ {} ì°¾ê¸° (ì‚¬ì¡± ì œê±°)
            start_idx = text.find('{')
            end_idx = text.rfind('}')

            if start_idx != -1 and end_idx != -1 and start_idx < end_idx:
                json_str = text[start_idx : end_idx + 1]
                return json.loads(json_str)

            # 5. ê´„í˜¸ê°€ ì—†ìœ¼ë©´ ì „ì²´ íŒŒì‹± ì‹œë„
            return json.loads(text)

        except Exception:
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë¹ˆ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
            return {}

    def _find_exact_position(self, full_text, target_snippet, start_from=0):
        """
        [Global Search í†µí•© ë²„ì „]
        1ë‹¨ê³„: ë‹¨ìˆœ ì¼ì¹˜ (Exact Match)
        2ë‹¨ê³„: ì •ê·œí™” ì¼ì¹˜ (Regex Normalization) - ê³µë°±/íŠ¹ìˆ˜ë¬¸ì ë¬´ì‹œ
        3ë‹¨ê³„: ìœ ì‚¬ë„ ì¼ì¹˜ (Fuzzy Match / Difflib) - ì˜¤íƒ€/ë³€í˜• ëŒ€ì‘
        """
        if not target_snippet:
            return -1, -1

        # ê²€ìƒ‰ ë²”ìœ„ë¥¼ start_from ì´í›„ë¡œ ì œí•œ
        search_scope_text = full_text[start_from:]

        # ---------------------------------------------------------
        # 1ë‹¨ê³„: ë‹¨ìˆœ ê²€ìƒ‰ (Exact Match)
        # ---------------------------------------------------------
        clean_target = target_snippet.strip(" '\"\n")
        if not clean_target:
            return -1, -1

        local_idx = search_scope_text.find(clean_target)
        if local_idx != -1:
            real_start = start_from + local_idx
            real_end = real_start + len(clean_target)
            return real_start, real_end

        # ---------------------------------------------------------
        # 2ë‹¨ê³„: ì •ê·œì‹ ê¸°ë°˜ ìœ ì—°í•œ ê²€ìƒ‰ (Normalization)
        # ---------------------------------------------------------
        # ê³µë°±, íŠ¹ìˆ˜ë¬¸ìë¥¼ ëª¨ë‘ ì œê±°í•˜ê³  ê¸€ì(Alphanumeric)ë§Œ ë¹„êµ
        def normalize(s):
            return re.sub(r'[\s\W_]+', '', s)

        norm_scope = normalize(search_scope_text)
        norm_target = normalize(clean_target)

        if not norm_target:
            return -1, -1

        norm_idx = norm_scope.find(norm_target)

        if norm_idx != -1:
            # ì •ì œëœ ì¸ë±ìŠ¤(norm_idx)ë¥¼ ì›ë³¸ ì¸ë±ìŠ¤ë¡œ ì—­ë§¤í•‘
            current_norm_pos = 0
            real_local_start = -1
            real_local_end = -1

            for i, char in enumerate(search_scope_text):
                # ì›ë³¸ ë¬¸ì ì¤‘ íŠ¹ìˆ˜ë¬¸ì/ê³µë°±ì€ ì¹´ìš´íŠ¸í•˜ì§€ ì•Šê³  ê±´ë„ˆëœ€
                if re.match(r'[\s\W_]', char):
                    continue

                # ì‹œì‘ ìœ„ì¹˜ í¬ì°©
                if current_norm_pos == norm_idx:
                    real_local_start = i

                # ë ìœ„ì¹˜ í¬ì°© (ê¸¸ì´ë§Œí¼ ì§„í–‰í–ˆì„ ë•Œ)
                if current_norm_pos == norm_idx + len(norm_target) - 1:
                    real_local_end = i + 1
                    break

                current_norm_pos += 1

            if real_local_start != -1 and real_local_end != -1:
                return (start_from + real_local_start), (start_from + real_local_end)

        # ---------------------------------------------------------
        # 3ë‹¨ê³„: ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰ (Fuzzy Match - Difflib)
        # ---------------------------------------------------------
        # ì—¬ê¸°ê¹Œì§€ ì™”ë‹¤ë©´ ì •ë°€ ê²€ìƒ‰ì— ì‹¤íŒ¨í•œ ê²ƒì„.
        # ìµœí›„ì˜ ìˆ˜ë‹¨ìœ¼ë¡œ 'ê°€ì¥ ë¹„ìŠ·í•œ ë¬¸ì¥'ì„ ì°¾ì•„ ë§¤ì¹­ ì‹œë„.

        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìª¼ê°œì„œ ë¹„êµ (ì†ë„ ìµœì í™”)
        # ë§ˆì¹¨í‘œ(.), ë¬¼ìŒí‘œ(?), ëŠë‚Œí‘œ(!), ì¤„ë°”ê¿ˆ(\n) ë“±ì„ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ”
        candidates = re.split(r'[.?!:\n]+', search_scope_text)

        best_ratio = 0
        best_candidate = ""

        for cand in candidates:
            # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥(5ê¸€ì ë¯¸ë§Œ)ì€ ë…¸ì´ì¦ˆì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
            if len(cand) < 5:
                continue

            # ìœ ì‚¬ë„ ê³„ì‚°
            ratio = difflib.SequenceMatcher(None, cand, clean_target).ratio()

            if ratio > best_ratio:
                best_ratio = ratio
                best_candidate = cand

        # ìœ ì‚¬ë„ê°€ 60% (0.6) ì´ìƒì¼ ë•Œë§Œ ì°¾ì€ ê²ƒìœ¼ë¡œ ê°„ì£¼
        if best_ratio >= 0.6:
            # ì°¾ì€ ë¬¸ì¥(best_candidate)ì´ ì›ë¬¸ì˜ ì–´ë””ì— ìˆëŠ”ì§€ ì°¾ê¸°
            # (splitë˜ë©´ì„œ íŠ¹ìˆ˜ë¬¸ìê°€ ì‚¬ë¼ì¡Œì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ findë¡œ ë‹¤ì‹œ ìœ„ì¹˜ ì¶”ì )
            fuzzy_idx = search_scope_text.find(best_candidate)
            if fuzzy_idx != -1:
                return (start_from + fuzzy_idx), (start_from + fuzzy_idx + len(best_candidate))

        # ëª¨ë“  ë°©ë²• ì‹¤íŒ¨
        return -1, -1

    def _double_check_batch_results(self, batch_items: List[Dict], first_results: Dict) -> Dict:
        """[2ì°¨ êµì°¨ ê²€ì¦] ID ê¸°ë°˜"""
        audit_payload = ""
        for item in batch_items:
            item_id = str(item['id'])
            f_res = first_results.get(item_id, {"is_positive": True, "reason": "Skip"})

            audit_payload += f"""
            ---
            [ID: {item_id}]
            - ëª…ì œ: {item['keyword']}
            - ì¦ê±°: {item['content'][:500]}
            - 1ì°¨ ê²°ë¡ : {"ì ì ˆ" if f_res.get("is_positive") else "ì˜¤ë¥˜"} ({f_res.get("reason")})
            """

        prompt = f"""
        ìµœì¢… ê°ìˆ˜ê´€ì…ë‹ˆë‹¤. 1ì°¨ íŒì •ì´ íƒ€ë‹¹í•œì§€ êµì°¨ ê²€ì¦í•˜ì„¸ìš”.
        íŠ¹íˆ 'ì˜¤ë¥˜'ë¡œ íŒì •ëœ ê±´ì´ ì§„ì§œ ì˜¤ë¥˜ì¸ì§€ ì‹ ì¤‘íˆ í™•ì¸í•˜ì„¸ìš”.

        [ì¶œë ¥ í˜•ì‹]
        IDë¥¼ í‚¤ë¡œ í•˜ëŠ” JSON ë°˜í™˜:
        {{
            "0": {{ "is_relevant": true, "is_positive": false, "reason": "ìµœì¢… ê·¼ê±°..." }}
        }}
        """
        try:
            response = self.llm.invoke([SystemMessage(content=prompt)])
            return self._clean_json_string(response.content)
        except Exception: return first_results


    def _retry_extract_sentence(self, chunk_text: str, keyword: str) -> str:
        """ì¬ì‹œë„: ë¬¸ì¥ ì¬ì¶”ì¶œ"""
        prompt = f"""
        ë‹¹ì‹ ì€ í…ìŠ¤íŠ¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤.
        ì•„ë˜ í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ '{keyword}'ê°€ í¬í•¨ëœ ë¬¸ì¥ì„ **í† ì”¨ í•˜ë‚˜ í‹€ë¦¬ì§€ ë§ê³  ê·¸ëŒ€ë¡œ** ì¶”ì¶œí•˜ì„¸ìš”.
        ë¬¸ì¥ì´ ë„ˆë¬´ ê¸¸ë©´ í•´ë‹¹ ë¶€ë¶„ë§Œ ì˜ë¼ì„œ ì¶œë ¥í•˜ê³ , ì—†ìœ¼ë©´ Noneì„ ì¶œë ¥í•˜ì„¸ìš”.
        """
        try:
            res = self.llm.invoke([
                SystemMessage(content=prompt),
                HumanMessage(content=f"Text: {chunk_text[:2500]}")
            ])
            val = res.content.strip().strip('"\'')
            return None if val == "None" or len(val) < 2 else val
        except: return None

    def _compress_text(self, text: str) -> str:
        """
        [í† í° ì ˆì•½] KoNLPy(Okt)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¡°ì‚¬/êµ¬ë‘ì  ì œê±° í›„ í•µì‹¬ í’ˆì‚¬ë§Œ ë‚¨ê¹€
        """
        try:
            from konlpy.tag import Okt
            okt = Okt()

            # ì‚´ë ¤ë‘˜ í’ˆì‚¬ (ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬, ë¶€ì‚¬, ìˆ«ì, ì•ŒíŒŒë²³)
            # Josa(ì¡°ì‚¬), Punctuation(êµ¬ë‘ì ) ë“±ì€ ì œê±°ë¨
            target_pos = ['Noun', 'Verb', 'Adjective', 'Adverb', 'Number', 'Alpha']

            # í˜•íƒœì†Œ ë¶„ì„ (stem=True: 'ë¨¹ì—ˆë‹¤' -> 'ë¨¹ë‹¤' ì›í˜• ë³µì›)
            tokens = okt.pos(text, stem=True)

            filtered_words = []
            for word, pos in tokens:
                if pos in target_pos:
                    filtered_words.append(word)
                # ë¶€ì •ì–´(Not)ëŠ” ì‚´ë ¤ì•¼ ê³ ì¦ ì˜¤ë¥˜ ë°©ì§€ ê°€ëŠ¥
                elif word in ["ì•ˆ", "ëª»", "ì—†ë‹¤", "ì•„ë‹ˆ"]:
                    filtered_words.append(word)

            return " ".join(filtered_words)

        except Exception as e:
            print(f"âš ï¸ í† í° ì••ì¶• ì‹¤íŒ¨ (KoNLPy ì—ëŸ¬): {e}")
            return text # ì‹¤íŒ¨í•˜ë©´ ì›ë¬¸ ê·¸ëŒ€ë¡œ ë°˜í™˜