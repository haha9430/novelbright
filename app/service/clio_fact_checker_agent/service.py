import json
import time
import re
from typing import List, Dict, Any, Set
import difflib

# LangChain & AI ê´€ë ¨
from langchain_upstage import ChatUpstage
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.utilities import GoogleSerperAPIWrapper
from langchain_community.tools.tavily_search import TavilySearchResults

# ë¡œì»¬ DB ë ˆí¬ì§€í† ë¦¬
from app.service.clio_fact_checker_agent.repo import ManuscriptRepository

class ManuscriptAnalyzer:
    def __init__(self, setting_path: str):
        # 1. LLM ì„¤ì • (Solar-pro)
        self.llm = ChatUpstage(model="solar-pro")

        # 2. ì†Œì„¤ ì„¤ì •(Plot DB) ë¡œë“œ -> í—ˆêµ¬ ì •ë³´ í•„í„°ë§ìš©
        self.settings = self._load_settings(setting_path)
        self.setting_keywords = self._extract_setting_keywords()

        # 3. ë¡œì»¬ ë²¡í„° DB (ê¸°ì¡´ ì§€ì‹)
        self.repo = ManuscriptRepository()

        # 4. Web Search ë„êµ¬ (tavily)
        # gl='kr': í•œêµ­ êµ¬ê¸€, hl='ko': í•œêµ­ì–´ ì¸í„°í˜ì´ìŠ¤ (í•„ìš”ì‹œ 'en'ìœ¼ë¡œ ë³€ê²½ ê°€ëŠ¥)
        self.search_tool = TavilySearchResults(k=5, search_depth="advanced")

        # 5. í…ìŠ¤íŠ¸ ë¶„í• ê¸°
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=2000,
            chunk_overlap=200,
            separators=["\n\n", "\n", ". ", " ", ""]
        )

    def _load_settings(self, path: str) -> Dict[str, Any]:
        """ì„¤ì • íŒŒì¼(JSON)ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"âš ï¸ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")
            return {}

    def _extract_setting_keywords(self) -> Set[str]:
        """ì†Œì„¤ ì† í—ˆêµ¬ì˜ ê³ ìœ ëª…ì‚¬(ë“±ì¥ì¸ë¬¼, ì§€ëª… ë“±)ë¥¼ Setìœ¼ë¡œ ì¶”ì¶œ"""
        keywords = set()
        data = self.settings

        # ë“±ì¥ì¸ë¬¼ ì´ë¦„
        for char in data.get("characters", []):
            name = char.get("name", "").strip()
            if name: keywords.add(name)

        # ì„¸ë ¥/ë‹¨ì²´ëª…
        factions = data.get("world_view", {}).get("factions", [])
        for f in factions:
            if isinstance(f, str):
                keywords.add(f.split("(")[0].strip())

        return keywords

    def analyze_manuscript(self, text: str) -> Dict[str, Any]:
        """
        [ë©”ì¸ ë¡œì§ - ìµœì í™” ë²„ì „]
        1. í…ìŠ¤íŠ¸ ë¶„í•  ë° ì¿¼ë¦¬ ì¶”ì¶œ
        2. ë¡œì»¬/ì›¹ ê²€ìƒ‰ ìˆ˜í–‰ (ì—¬ê¸°ê¹Œì§€ëŠ” ê°œë³„ ìˆ˜í–‰)
        3. ê²€ìƒ‰ ê²°ê³¼ë¥¼ ëª¨ì•„ì„œ LLMì— ì¼ê´„ ê²€ì¦ ìš”ì²­ (Batch Processing)
        """
        print(f"ğŸ“„ ì›ê³  ë¶„ì„ ì‹œì‘ (ì´ {len(text)}ì)")

        # 1. í…ìŠ¤íŠ¸ ë¶„í•  ë° ì¿¼ë¦¬ ì¶”ì¶œ (ê¸°ì¡´ ë™ì¼)
        chunks = self.text_splitter.split_text(text)
        all_query_items = {}

        for i, chunk in enumerate(chunks):
            items = self._extract_search_queries(chunk)
            for item in items:
                kw = item['keyword']
                origin_snippet = item.get('original_sentence', '')

                # ìœ„ì¹˜ ì°¾ê¸° (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
                start_idx, end_idx = self._find_exact_position(
                    full_text=text,
                    target_snippet=origin_snippet,
                    start_from=0
                )

                # ê²€ì¦ ë¡œì§ ì‹œì‘
                def _is_content_equal(text1, text2):
                    """íŠ¹ìˆ˜ë¬¸ì/ê³µë°± ì œê±° í›„ ë‚´ìš© ì¼ì¹˜ ì—¬ë¶€ í™•ì¸"""
                    def normalize(s):
                        return re.sub(r'[\s\W_]+', '', s)
                    return normalize(text1) == normalize(text2)

                def _retry_extract_sentence(chunk_text, keyword):
                    """
                    [LLM ì¬ìš”ì²­] íŠ¹ì • í‚¤ì›Œë“œì— ëŒ€í•´ ë¬¸ì¥ ì¶”ì¶œë§Œ ë‹¤ì‹œ ìˆ˜í–‰
                    """
                    prompt = f"""
                    ë‹¹ì‹ ì€ í…ìŠ¤íŠ¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤.
                    ì•„ë˜ í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ '{keyword}'ê°€ í¬í•¨ëœ ë¬¸ì¥ì„ **í† ì”¨ í•˜ë‚˜ í‹€ë¦¬ì§€ ë§ê³  ê·¸ëŒ€ë¡œ** ì¶”ì¶œí•˜ì„¸ìš”.

                    [ê·œì¹™]
                    1. ë¬¸ì¥ì´ ë„ˆë¬´ ê¸¸ë©´, í‚¤ì›Œë“œ ì£¼ë³€ 10ì–´ì ˆë§Œ ì˜ë¼ì„œ ê°€ì ¸ì˜¤ì„¸ìš”.
                    2. ì„¤ëª…ì´ë‚˜ ìˆ˜ì‹ì–´ë¥¼ ë¶™ì´ì§€ ë§ê³  ì˜¤ì§ **ë³¸ë¬¸ ë‚´ìš©ë§Œ** ì¶œë ¥í•˜ì„¸ìš”.
                    3. ì—†ìœ¼ë©´ 'None'ì´ë¼ê³ ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
                    """

                    try:
                        response = self.llm.invoke([
                            SystemMessage(content=prompt),
                            HumanMessage(content=f"Text: {chunk_text[:3000]}") # ë¬¸ë§¥ ì œê³µ
                        ])
                        result = response.content.strip().strip('"\'')

                        if result == "None" or len(result) < 2:
                            return None
                        return result

                    except Exception as e:
                        print(f"âš ï¸ ì¬ì‹œë„ ì¤‘ ì—ëŸ¬: {e}")
                        return None

                is_match_success = False

                if start_idx != -1:
                    actual_found_text = text[start_idx:end_idx]

                    # 1. ì™„ë²½ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
                    if actual_found_text == origin_snippet:
                        is_match_success = True
                    else:
                        # 2. [ë¶ˆì¼ì¹˜ ë°œìƒ] -> ì •ê·œí™”(Normalization) í›„ ì¬ë¹„êµ
                        # ê³µë°±, ì¤„ë°”ê¿ˆ, íŠ¹ìˆ˜ë¬¸ìë¥¼ ë‹¤ ë–¼ê³  ë¹„êµí•´ì„œ ê¸€ì ì•Œë§¹ì´ê°€ ê°™ì€ì§€ í™•ì¸
                        if _is_content_equal(actual_found_text, origin_snippet):
                            print(f"   âš ï¸ [ë³´ì • ì„±ê³µ] ë¬¸ì¥ì€ ë‹¤ë¥´ì§€ë§Œ ë‚´ìš©ì€ ê°™ìŠµë‹ˆë‹¤.")
                            print(f"       LLM: {repr(origin_snippet)}")
                            print(f"       Raw: {repr(actual_found_text)}")
                            is_match_success = True
                        else:
                            print(f"   âŒ [ë¶ˆì¼ì¹˜] ìœ„ì¹˜ëŠ” ì°¾ì•˜ìœ¼ë‚˜ ë‚´ìš©ì´ ë„ˆë¬´ ë‹¤ë¦…ë‹ˆë‹¤.")
                            # ì—¬ê¸°ì„œ ì¬ì‹œë„ ë¡œì§ì„ ìˆ˜í–‰í•˜ê±°ë‚˜, ê·¸ëƒ¥ ì´ ìœ„ì¹˜ë¥¼ ì‹ ë¢°í• ì§€ ê²°ì •
                            # ë³´í†µ _find_exact_positionì´ 3ë‹¨ê³„(ìœ ì‚¬ë„)ê¹Œì§€ ê°”ë‹¤ë©´,
                            # ì‹¤ì œë¡œëŠ” ë§ëŠ” ìœ„ì¹˜ì¼ í™•ë¥ ì´ ë†’ìŒ.

                # 3. [ì¬ì‹œë„ ë¡œì§] ìœ„ì¹˜ë¥¼ ì•„ì˜ˆ ëª» ì°¾ì•˜ê±°ë‚˜, ì°¾ì•˜ëŠ”ë° ë‚´ìš©ì´ ì˜ ë”´íŒì¸ ê²½ìš°
                if start_idx == -1 or (start_idx != -1 and not is_match_success):
                    print(f"   ğŸ”„ [ì¬ì‹œë„] '{kw}'ì— ëŒ€í•œ ë¬¸ì¥ ì¶”ì¶œì„ ë‹¤ì‹œ ì‹œë„í•©ë‹ˆë‹¤...")

                    # LLMì—ê²Œ í•´ë‹¹ í‚¤ì›Œë“œë¡œ ë‹¤ì‹œ ë¬¸ì¥ì„ ë½‘ì•„ë‹¬ë¼ê³  ìš”ì²­ (Retry í•¨ìˆ˜ í˜¸ì¶œ)
                    new_snippet = _retry_extract_sentence(chunk, kw)

                    if new_snippet:
                        print(f"      -> ì¬ì¶”ì¶œëœ ë¬¸ì¥: {new_snippet}")
                        # ë‹¤ì‹œ ìœ„ì¹˜ ì°¾ê¸° ì‹œë„
                        start_idx, end_idx = self._find_exact_position(text, new_snippet, 0)

                        if start_idx != -1:
                            print(f"      âœ… ì¬ì‹œë„ ì„±ê³µ! ìœ„ì¹˜ ì°¾ìŒ.")
                            item['original_sentence'] = new_snippet # ì—…ë°ì´íŠ¸

                        if start_idx != -1:
                            item['start_index'] = start_idx
                            item['end_index'] = end_idx
                        else:
                            item['start_index'] = -1
                            item['end_index'] = -1

                        all_query_items[kw] = item

                        print(f"   -> ì´ {len(all_query_items)}ê°œì˜ ê²€ìƒ‰ í›„ë³´ ì¶”ì¶œë¨")

                        known_settings = []
                        historical_context = []

                        # [ë³€ê²½ì  1] ê²€ì¦ ëŒ€ê¸°ì—´ ìƒì„±
                        verification_queue = []

                        # 2. ê²€ìƒ‰ ìˆ˜í–‰ (ê²€ì¦ì€ í•˜ì§€ ì•Šê³  ë°ì´í„°ë§Œ ëª¨ìŒ)
                        for keyword, item_data in all_query_items.items():
                            query_string = item_data['search_query']
                            origin_sent = item_data.get('original_sentence', '')

                            # í—ˆêµ¬ í•„í„°ë§
                            is_fiction = False
                            for fiction_term in self.setting_keywords:
                                if fiction_term in keyword or keyword in fiction_term:
                                    is_fiction = True
                                    break

                            if is_fiction:
                                known_settings.append(keyword)
                                continue

                            print(f"ğŸ” ê²€ìƒ‰ ìˆ˜í–‰: '{keyword}'")

                            # Step A: ë¡œì»¬ DB
                            search_data = self._check_local_db(keyword)

                            # Step B: ì›¹ ê²€ìƒ‰
                            if not search_data:
                                search_data = self._search_web(query_string)
                                time.sleep(0.1)  # ê²€ìƒ‰ API ì†ë„ ì¡°ì ˆ

                            # Step C: ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆë‹¤ë©´ íì— ì ì¬
                            if search_data:
                                # ê²€ì¦ì— í•„ìš”í•œ ëª¨ë“  ì •ë³´ë¥¼ íŒ¨í‚¤ì§•
                                verification_queue.append({
                                    "keyword": keyword,
                                    "query": query_string,
                                    "content": search_data['content'],  # ê²€ìƒ‰ëœ ê¸´ ë³¸ë¬¸
                                    "context": origin_sent,  # ì†Œì„¤ ì† ì›ë¬¸ ë¬¸ì¥
                                    "item_data": item_data,  # ì›ë³¸ ì•„ì´í…œ ë°ì´í„° (ìœ„ì¹˜ ì •ë³´ ë“±)
                                    "search_source": search_data.get('source', 'Unknown')
                                })

                        # 3. [ë³€ê²½ì  2] ì¼ê´„ ê²€ì¦ (Batch Verification)
                        if verification_queue:
                            print(f"ğŸš€ ì´ {len(verification_queue)}ê±´ì— ëŒ€í•´ ì¼ê´„ íŒ©íŠ¸ì²´í¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤...")

                            # ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì„¤ì • (í•œ ë²ˆì— ë„ˆë¬´ ë§ì´ ë³´ë‚´ë©´ í† í° ì´ˆê³¼/ë‹µë³€ í’ˆì§ˆ ì €í•˜ ìš°ë ¤)
                            BATCH_SIZE = 5

                            for i in range(0, len(verification_queue), BATCH_SIZE):
                                batch_items = verification_queue[i: i + BATCH_SIZE]
                                print(f"   -> Batch {i // BATCH_SIZE + 1} ì²˜ë¦¬ ì¤‘ ({len(batch_items)}ê±´)...")

                                # LLM í˜¸ì¶œ
                                verified_results = self._verify_batch_relevance(batch_items)

                                # ê²°ê³¼ ë§¤í•‘
                                for item in batch_items:
                                    kw = item['keyword']

                                    # LLM ê²°ê³¼ì—ì„œ í•´ë‹¹ í‚¤ì›Œë“œì— ëŒ€í•œ ê²€ì¦ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
                                    ver_res = verified_results.get(kw)

                                    if ver_res and ver_res.get('is_relevant') and ver_res.get('is_positive') is not None:
                                        # ê²€ì¦ ê²°ê³¼ê°€ ìœ íš¨í•œ ê²½ìš°ë§Œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                                        final_obj = {
                                            "keyword": kw,
                                            "content": item['content'],
                                            "source": item['search_source'],
                                            "is_relevant": True,
                                            "is_positive": ver_res['is_positive'],
                                            "reason": ver_res['reason'],
                                            "original_sentence": item['context'],
                                            "start_index": item['item_data'].get('start_index'),
                                            "end_index": item['item_data'].get('end_index')
                                        }
                                        historical_context.append(final_obj)

                                        status = "âœ… í†µê³¼" if ver_res['is_positive'] else "âš ï¸ ì˜¤ë¥˜ ì˜ì‹¬"
                                        print(f"      [{status}] {kw}: {ver_res['reason']}")
                                    else:
                                        print(f"      [ğŸ—‘ï¸ íƒˆë½] {kw}: ê´€ë ¨ ì—†ìŒ í˜¹ì€ ë°ì´í„° ë¶€ì¡±")

                        return {
                            "found_entities_count": len(all_query_items),
                            "setting_terms_found": list(set(known_settings)),
                            "historical_context": historical_context
                        }

    def _extract_search_queries(self, text: str) -> List[Dict[str, str]]:
        """
        [ìˆ˜ì •ë¨] ë‹¨ìˆœ ëª…ì‚¬ê°€ ì•„ë‹Œ 'ì—­ì‚¬ì  ì‚¬ì‹¤ ê´€ê³„(ëª…ì œ)'ì™€ 'ì‹œëŒ€ì  ì •í•©ì„±'ì„ ê²€ì¦í•˜ëŠ” ì¿¼ë¦¬ ìƒì„±ê¸°
        """
        prompt = """
        ë‹¹ì‹ ì€ ì—­ì‚¬ ì†Œì„¤ì˜ ê³ ì¦ ì˜¤ë¥˜ë¥¼ ì°¾ì•„ë‚´ëŠ” 'íŒ©íŠ¸ì²´í¬ ì¿¼ë¦¬ ì„¤ê³„ì'ì…ë‹ˆë‹¤.
        ë‹¨ìˆœí•œ ê³ ìœ ëª…ì‚¬ ì¶”ì¶œì´ ì•„ë‹ˆë¼, **"ì´ ë‚´ìš©ì´ ì—­ì‚¬ì ìœ¼ë¡œ ê°€ëŠ¥í•œê°€?"**ë¥¼ ê²€ì¦í•˜ê¸° ìœ„í•œ **ëª…ì œ(Proposition)ì™€ ë§¥ë½**ì„ ì¶”ì¶œí•˜ì„¸ìš”.

        [ì¶”ì¶œ ê¸°ì¤€: ë¬´ì—‡ì„ ê²€ì¦í•´ì•¼ í•˜ëŠ”ê°€?]
        1. **í–‰ìœ„ì™€ ì‚¬ê±´ì˜ ì‚¬ì‹¤ì„± (Historical Plausibility):**
           - ì‹¤ì¡´ ì¸ë¬¼ì´ í•´ë‹¹ ì‹œì ì— ê·¸ ì¥ì†Œì— ìˆì—ˆê±°ë‚˜, ê·¸ í–‰ë™ì„ í–ˆëŠ”ì§€.
        2. **ì‹œëŒ€ì  ë¶ˆì¼ì¹˜ (Anachronism):**
           - ë“±ì¥í•œ ë¬¼ê±´, ìš©ì–´, ê°œë…ì´ í•´ë‹¹ ì‹œëŒ€ì— ì¡´ì¬í–ˆëŠ”ì§€.
        3. **ë¬¸í™”/ì œë„ì  ë°°ê²½ (Cultural Context):**
           - ì˜ë³µ, ì‹ì‚¬, ì˜ë£Œ í–‰ìœ„, ë²•ë¥  ë“±ì´ ë‹¹ì‹œ ê³ ì¦ì— ë§ëŠ”ì§€.

        [ì œì™¸ ëŒ€ìƒ (Negative Rules)]
        - ì—­ì‚¬ì  ë§¥ë½ì´ ì—†ëŠ” ë‹¨ìˆœí•œ ì¼ìƒ ë¬˜ì‚¬ (ì˜ˆ: "ë°¥ì„ ë¨¹ì—ˆë‹¤", "ì ì„ ì¤ë‹¤").
        - ìˆ˜ì‹ì–´ê°€ ì—†ëŠ” ì¼ë°˜ ëª…ì‚¬ ë‹¨ë… ì¶”ì¶œ ê¸ˆì§€ (ì˜ˆ: 'ë³‘ì›', 'ì‚¬ëŒ', 'í•˜ëŠ˜' -> ì ˆëŒ€ ê¸ˆì§€).
        - **ë°˜ë“œì‹œ 'ê²€ì¦ì´ í•„ìš”í•œ êµ¬ì²´ì  ì„œìˆ 'ì´ í¬í•¨ëœ ê²½ìš°ë§Œ ì¶”ì¶œ.**

        [ì¶œë ¥ í˜•ì‹]
        ë°˜ë“œì‹œ ì•„ë˜ì™€ ê°™ì€ **JSON ë¦¬ìŠ¤íŠ¸**ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
        [
            {
                "keyword": "ê²€ì¦ ëŒ€ìƒ (ì§§ì€ êµ¬ í˜¹ì€ ì£¼ì–´+ì„œìˆ ì–´ ìš”ì•½)",
                "original_sentence": "ë³¸ë¬¸ì—ì„œ í† ì”¨ í•˜ë‚˜ ì•ˆ ë°”ê¾¸ê³  ê·¸ëŒ€ë¡œ ë³µì‚¬í•œ ë¬¸ì¥ ì „ì²´",
                "search_query": "êµ¬ê¸€/ìœ„í‚¤í”¼ë””ì•„ ê²€ìƒ‰ì„ ìœ„í•œ ì¿¼ë¦¬ (ì‹œëŒ€ í‚¤ì›Œë“œ í¬í•¨)",
                "reason": "ì´ í•­ëª©ì„ ì—­ì‚¬ì ìœ¼ë¡œ ê²€ì¦í•´ì•¼ í•˜ëŠ” êµ¬ì²´ì ì¸ ì´ìœ "
            }
        ]
        """

        try:
            # LLMì—ê²Œ í…ìŠ¤íŠ¸ ì „ë‹¬
            response = self.llm.invoke([
                SystemMessage(content=prompt),
                HumanMessage(content=f"Text: {text[:3500]}") # ë¬¸ë§¥ íŒŒì•…ì„ ìœ„í•´ ê¸¸ì´ ì•½ê°„ ëŠ˜ë¦¼
            ])
            content = response.content.strip()

            # JSON íŒŒì‹±
            return self._parse_json_garbage(content)

        except Exception as e:
            print(f"âš ï¸ ì¿¼ë¦¬ ìƒì„± ì—ëŸ¬: {e}")
            return []

    def _check_local_db(self, keyword: str) -> Dict[str, Any]:
        """ë¡œì»¬ ë²¡í„° DB ì¡°íšŒ"""
        try:
            # ê²€ìƒ‰
            search_result = self.repo.search(query_text=keyword, n_results=1)

            if not search_result['documents'][0]:
                return None

            dist = search_result['distances'][0][0]
            content = search_result['documents'][0][0]

            # ê±°ë¦¬ ì„ê³„ê°’ (1.0ë³´ë‹¤ ê°€ê¹Œì›Œì•¼ ê´€ë ¨ì„± ìˆìŒ)
            if dist < 1.0:
                return {
                    "keyword": keyword,
                    "content": content,
                    "source": "Local History DB",
                    "confidence": round(1 - (dist/2), 2)
                }
            return None
        except Exception:
            return None

    def _search_web(self, query: str) -> Dict[str, Any]:
        """Tavily AI ì›¹ ê²€ìƒ‰"""
        try:
            # ê²€ìƒ‰ì–´ ë³´ì • (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
            if "ì—­ì‚¬" not in query and "history" not in query.lower():
                final_query = f"{query} ì—­ì‚¬ history"
            else:
                final_query = query

            # Tavily ê²€ìƒ‰ ì‹¤í–‰ (ê²°ê³¼ëŠ” ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë°˜í™˜ë¨)
            # [{'url': '...', 'content': '...'}, ...]
            search_results = self.search_tool.run(final_query)

            if not search_results:
                return None

            # ì—¬ëŸ¬ ê°œì˜ ê²€ìƒ‰ ê²°ê³¼ ë³¸ë¬¸ì„ í•˜ë‚˜ë¡œ í•©ì¹¨
            combined_content = "\n\n".join([
                f"[Source: {res['url']}]\n{res['content']}"
                for res in search_results
            ])

            return {
                "keyword": query,
                "content": combined_content,
                "source": "Web Search (Tavily AI)"
            }
        except Exception as e:
            print(f"âš ï¸ Tavily ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None

    def _verify_batch_relevance(self, batch_items: List[Dict]) -> Dict[str, Dict]:
        """
        [1ì°¨ ê²€ì¦] ì—¬ëŸ¬ ê±´ì˜ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í•œ ë²ˆì— ê²€ì¦í•˜ëŠ” í•¨ìˆ˜
        """
        items_text = ""
        for idx, item in enumerate(batch_items):
            items_text += f"""
            ---
            [í•­ëª© {idx+1}]
            - í‚¤ì›Œë“œ(ID): {item['keyword']}
            - ì†Œì„¤ ì† ë§¥ë½: {item['context']}
            - ê²€ìƒ‰ ê²°ê³¼: {item['content'][:800]} ... (ìƒëµ)
            """

        prompt = f"""
        ë‹¹ì‹ ì€ ì—­ì‚¬ ì†Œì„¤ íŒ©íŠ¸ì²´ì»¤ì…ë‹ˆë‹¤. ì•„ë˜ {len(batch_items)}ê°œì˜ í•­ëª©ì„ ê²€í† í•˜ì—¬ JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”.

        [ì…ë ¥ ë°ì´í„°]
        {items_text}

        [íŒë‹¨ ê¸°ì¤€]
        1. **is_relevant**: ê²€ìƒ‰ ê²°ê³¼ê°€ í•´ë‹¹ í‚¤ì›Œë“œì˜ ì—­ì‚¬/ì •ë³´ í™•ì¸ì— ìœ íš¨í•œ ìë£Œì¸ê°€? (ê´‘ê³ /ë¬´ê´€í•˜ë©´ false)
        2. **is_positive**: 
           - ì—­ì‚¬ì  ì‚¬ì‹¤ê³¼ ì¼ì¹˜í•˜ê±°ë‚˜ ê°œì—°ì„±ì´ ìˆìœ¼ë©´ true.
           - ëª…ë°±í•œ ì˜¤ë¥˜(ì‹œëŒ€ì°©ì˜¤ ë“±)ë©´ false.
           - íŒë‹¨ ë³´ë¥˜ ì‹œ true.

        [ì¶œë ¥ í˜•ì‹]
        ë°˜ë“œì‹œ ì•„ë˜ì™€ ê°™ì€ **JSON ê°ì²´**ë¡œ ë°˜í™˜í•˜ì„¸ìš”. í‚¤(Key)ëŠ” ê° í•­ëª©ì˜ 'í‚¤ì›Œë“œ(ID)'ì—¬ì•¼ í•©ë‹ˆë‹¤.

        {{
            "í‚¤ì›Œë“œ1": {{ "is_relevant": true, "is_positive": true, "reason": "ê·¼ê±° ìš”ì•½" }},
            "í‚¤ì›Œë“œ2": {{ "is_relevant": false, "is_positive": false, "reason": "ê´€ë ¨ ì—†ëŠ” ìë£Œì„" }}
        }}
        """

        try:
            response = self.llm.invoke([SystemMessage(content=prompt)])
            # ê¸°ì¡´ì— ì •ì˜ëœ _clean_json_string ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒì‹±
            return self._clean_json_string(response.content)
        except Exception as e:
            print(f"âš ï¸ 1ì°¨ ë°°ì¹˜ ê²€ì¦ ì¤‘ ì—ëŸ¬: {e}")
            return {}

    def _parse_json_garbage(self, text: str) -> List[Dict]:
        """LLMì´ ì£¼ëŠ” ì§€ì €ë¶„í•œ JSON ë¬¸ìì—´ì—ì„œ ë¦¬ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ"""
        try:
            # ë§ˆí¬ë‹¤ìš´ ì œê±°
            text = text.replace("```json", "").replace("```", "").strip()

            # ê°€ì¥ ë°”ê¹¥ìª½ ëŒ€ê´„í˜¸ ì°¾ê¸°
            start = text.find('[')
            end = text.rfind(']')
            if start != -1 and end != -1:
                json_str = text[start:end+1]
                return json.loads(json_str)
            return []
        except:
            return []

    def _clean_json_string(self, text: str) -> str:
        """
                [ìˆ˜ì •ë¨] ì…ë ¥ì´ ì´ë¯¸ Dictë¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ê³ ,
                Stringì´ë¼ë©´ JSON êµ¬ê°„ë§Œ ì¶”ì¶œí•˜ì—¬ íŒŒì‹±í•©ë‹ˆë‹¤.
                (TypeError ë°©ì§€ìš© ë°©ì–´ ì½”ë“œ í¬í•¨)
                """
        # 1. ì…ë ¥ì´ ì´ë¯¸ ë”•ì…”ë„ˆë¦¬(Dict)ë¼ë©´ íŒŒì‹±í•  í•„ìš” ì—†ì´ ë°”ë¡œ ë°˜í™˜
        if isinstance(text, dict):
            return text

        # 2. ë¬¸ìì—´ì´ ì•„ë‹ˆë¼ë©´(None ë“±) ë¹ˆ Dict ë°˜í™˜
        if not isinstance(text, str):
            return {}

        try:
            # 3. ë§ˆí¬ë‹¤ìš´ ë° ê³µë°± ì œê±°
            text = text.replace("```json", "").replace("```", "").strip()

            # 4. ê°€ì¥ ë°”ê¹¥ìª½ {} ì°¾ê¸° (ì‚¬ì¡± ì œê±°)
            start_idx = text.find('{')
            end_idx = text.rfind('}')

            if start_idx != -1 and end_idx != -1 and start_idx < end_idx:
                json_str = text[start_idx : end_idx + 1]
                return json.loads(json_str)

            # 5. ê´„í˜¸ê°€ ì—†ìœ¼ë©´ ì „ì²´ íŒŒì‹± ì‹œë„
            return json.loads(text)

        except Exception:
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë¹ˆ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
            return {}

    def _find_exact_position(self, full_text, target_snippet, start_from=0):
        """
        [Global Search í†µí•© ë²„ì „]
        1ë‹¨ê³„: ë‹¨ìˆœ ì¼ì¹˜ (Exact Match)
        2ë‹¨ê³„: ì •ê·œí™” ì¼ì¹˜ (Regex Normalization) - ê³µë°±/íŠ¹ìˆ˜ë¬¸ì ë¬´ì‹œ
        3ë‹¨ê³„: ìœ ì‚¬ë„ ì¼ì¹˜ (Fuzzy Match / Difflib) - ì˜¤íƒ€/ë³€í˜• ëŒ€ì‘
        """
        if not target_snippet:
            return -1, -1

        # ê²€ìƒ‰ ë²”ìœ„ë¥¼ start_from ì´í›„ë¡œ ì œí•œ
        search_scope_text = full_text[start_from:]

        # ---------------------------------------------------------
        # 1ë‹¨ê³„: ë‹¨ìˆœ ê²€ìƒ‰ (Exact Match)
        # ---------------------------------------------------------
        clean_target = target_snippet.strip(" '\"\n")
        if not clean_target:
            return -1, -1

        local_idx = search_scope_text.find(clean_target)
        if local_idx != -1:
            real_start = start_from + local_idx
            real_end = real_start + len(clean_target)
            return real_start, real_end

        # ---------------------------------------------------------
        # 2ë‹¨ê³„: ì •ê·œì‹ ê¸°ë°˜ ìœ ì—°í•œ ê²€ìƒ‰ (Normalization)
        # ---------------------------------------------------------
        # ê³µë°±, íŠ¹ìˆ˜ë¬¸ìë¥¼ ëª¨ë‘ ì œê±°í•˜ê³  ê¸€ì(Alphanumeric)ë§Œ ë¹„êµ
        def normalize(s):
            return re.sub(r'[\s\W_]+', '', s)

        norm_scope = normalize(search_scope_text)
        norm_target = normalize(clean_target)

        if not norm_target:
            return -1, -1

        norm_idx = norm_scope.find(norm_target)

        if norm_idx != -1:
            # ì •ì œëœ ì¸ë±ìŠ¤(norm_idx)ë¥¼ ì›ë³¸ ì¸ë±ìŠ¤ë¡œ ì—­ë§¤í•‘
            current_norm_pos = 0
            real_local_start = -1
            real_local_end = -1

            for i, char in enumerate(search_scope_text):
                # ì›ë³¸ ë¬¸ì ì¤‘ íŠ¹ìˆ˜ë¬¸ì/ê³µë°±ì€ ì¹´ìš´íŠ¸í•˜ì§€ ì•Šê³  ê±´ë„ˆëœ€
                if re.match(r'[\s\W_]', char):
                    continue

                # ì‹œì‘ ìœ„ì¹˜ í¬ì°©
                if current_norm_pos == norm_idx:
                    real_local_start = i

                # ë ìœ„ì¹˜ í¬ì°© (ê¸¸ì´ë§Œí¼ ì§„í–‰í–ˆì„ ë•Œ)
                if current_norm_pos == norm_idx + len(norm_target) - 1:
                    real_local_end = i + 1
                    break

                current_norm_pos += 1

            if real_local_start != -1 and real_local_end != -1:
                return (start_from + real_local_start), (start_from + real_local_end)

        # ---------------------------------------------------------
        # 3ë‹¨ê³„: ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰ (Fuzzy Match - Difflib)
        # ---------------------------------------------------------
        # ì—¬ê¸°ê¹Œì§€ ì™”ë‹¤ë©´ ì •ë°€ ê²€ìƒ‰ì— ì‹¤íŒ¨í•œ ê²ƒì„.
        # ìµœí›„ì˜ ìˆ˜ë‹¨ìœ¼ë¡œ 'ê°€ì¥ ë¹„ìŠ·í•œ ë¬¸ì¥'ì„ ì°¾ì•„ ë§¤ì¹­ ì‹œë„.

        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìª¼ê°œì„œ ë¹„êµ (ì†ë„ ìµœì í™”)
        # ë§ˆì¹¨í‘œ(.), ë¬¼ìŒí‘œ(?), ëŠë‚Œí‘œ(!), ì¤„ë°”ê¿ˆ(\n) ë“±ì„ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ”
        candidates = re.split(r'[.?!:\n]+', search_scope_text)

        best_ratio = 0
        best_candidate = ""

        for cand in candidates:
            # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥(5ê¸€ì ë¯¸ë§Œ)ì€ ë…¸ì´ì¦ˆì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
            if len(cand) < 5:
                continue

            # ìœ ì‚¬ë„ ê³„ì‚°
            ratio = difflib.SequenceMatcher(None, cand, clean_target).ratio()

            if ratio > best_ratio:
                best_ratio = ratio
                best_candidate = cand

        # ìœ ì‚¬ë„ê°€ 60% (0.6) ì´ìƒì¼ ë•Œë§Œ ì°¾ì€ ê²ƒìœ¼ë¡œ ê°„ì£¼
        if best_ratio >= 0.6:
            # ì°¾ì€ ë¬¸ì¥(best_candidate)ì´ ì›ë¬¸ì˜ ì–´ë””ì— ìˆëŠ”ì§€ ì°¾ê¸°
            # (splitë˜ë©´ì„œ íŠ¹ìˆ˜ë¬¸ìê°€ ì‚¬ë¼ì¡Œì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ findë¡œ ë‹¤ì‹œ ìœ„ì¹˜ ì¶”ì )
            fuzzy_idx = search_scope_text.find(best_candidate)
            if fuzzy_idx != -1:
                return (start_from + fuzzy_idx), (start_from + fuzzy_idx + len(best_candidate))

        # ëª¨ë“  ë°©ë²• ì‹¤íŒ¨
        return -1, -1

    def _double_check_batch_results(self, batch_items: List[Dict], first_results: Dict) -> Dict:
        """
        [2ì°¨ ê²€ì¦] 1ì°¨ ê²€ì¦ ê²°ê³¼ë¥¼ ë¹„íŒì ìœ¼ë¡œ ì¬ê²€í† í•˜ëŠ” êµì°¨ ê²€ì¦ í•¨ìˆ˜
        """
        audit_payload = ""
        for idx, item in enumerate(batch_items):
            kw = item['keyword']
            f_res = first_results.get(kw, {"is_positive": True, "reason": "1ì°¨ ê²€ì¦ ë°ì´í„° ëˆ„ë½", "is_relevant": True})

            audit_payload += f"""
            ---
            [í•­ëª© {idx+1}: {kw}]
            - ì†Œì„¤ ë§¥ë½: {item['context']}
            - ê²€ìƒ‰ ì¦ê±°: {item['content'][:600]}...
            - 1ì°¨ íŒë‹¨ ê²°ê³¼: {"ì¼ì¹˜" if f_res.get("is_positive") else "ì˜¤ë¥˜"}
            - 1ì°¨ íŒë‹¨ ê·¼ê±°: {f_res.get("reason")}
            """

        prompt = f"""
        ë‹¹ì‹ ì€ ì—­ì‚¬ ì†Œì„¤ ê³ ì¦ì˜ ìµœì¢… ê°ìˆ˜ê´€ì…ë‹ˆë‹¤. 
        1ì°¨ íŒ©íŠ¸ì²´ì»¤ê°€ ë‚´ë¦° ê²°ë¡ ì´ 'ê²€ìƒ‰ ì¦ê±°'ì™€ 'ì†Œì„¤ ë§¥ë½'ì— ë¹„ì¶”ì–´ ë³¼ ë•Œ ì •ë§ íƒ€ë‹¹í•œì§€ ì¬ê²€í† í•˜ì‹­ì‹œì˜¤.
        ë§Œì•½ 1ì°¨ íŒë‹¨ì´ í‹€ë ¸ë‹¤ë©´ ì´ë¥¼ ë°”ë¡œì¡ê³ , ë§ë‹¤ë©´ ê·¼ê±°ë¥¼ ë³´ê°•í•˜ì‹­ì‹œì˜¤.

        [ì…ë ¥ ë°ì´í„°]
        {audit_payload}

        [ì¶œë ¥ í˜•ì‹]
        ë°˜ë“œì‹œ ê° í•­ëª©ì˜ í‚¤ì›Œë“œë¥¼ í‚¤(Key)ë¡œ í•˜ëŠ” JSON ê°ì²´ë¡œ ë°˜í™˜í•˜ì„¸ìš”.
        {{
            "í‚¤ì›Œë“œ": {{ "is_relevant": true, "is_positive": true/false, "reason": "ìµœì¢… í™•ì •ëœ ê³ ì¦ ê·¼ê±°" }}
        }}
        """

        try:
            response = self.llm.invoke([SystemMessage(content=prompt)])
            return self._clean_json_string(response.content)
        except Exception as e:
            print(f"âš ï¸ 2ì°¨ êµì°¨ ê²€ì¦ ì¤‘ ì—ëŸ¬: {e}")
            return first_results  # ì—ëŸ¬ ì‹œ 1ì°¨ ê²°ê³¼ ë°˜í™˜