import json
from typing import List, Dict, Any, Set
from langchain_upstage import ChatUpstage
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_text_splitters import RecursiveCharacterTextSplitter

# [ë³€ê²½] VectorService ëŒ€ì‹  ë°©ê¸ˆ ë§Œë“  Repo ì„í¬íŠ¸
from app.service.manuscript.repo import ManuscriptRepository

class ManuscriptAnalyzer:
    # [ë³€ê²½] ì´ˆê¸°í™” ì‹œ repoë¥¼ ì£¼ì…ë°›ê±°ë‚˜ ë‚´ë¶€ì—ì„œ ìƒì„±
    def __init__(self, setting_path: str):
        self.llm = ChatUpstage(model="solar-pro")
        self.settings = self._load_settings(setting_path)
        self.setting_keywords = self._extract_setting_keywords()

        # [ë³€ê²½] ì„ì‹œ Repo ì§ì ‘ ì—°ê²°
        self.repo = ManuscriptRepository()

        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=2000,
            chunk_overlap=200,
            separators=["\n\n", "\n", ". ", " ", ""]
        )

    def _load_settings(self, path: str) -> Dict[str, Any]:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)

    def _extract_setting_keywords(self) -> Set[str]:
        keywords = set()
        data = self.settings
        for char in data.get("characters", []):
            keywords.add(char["name"])
        factions = data.get("world_view", {}).get("factions", [])
        for f in factions:
            keywords.add(f.split("(")[0].strip())
        keywords.add("ë²½ë ¥")
        keywords.add("ì´ë§¤ë§ëŸ‰")
        return keywords

    def analyze_manuscript(self, text: str) -> Dict[str, Any]:
        print(f"ğŸ“„ ì›ê³  ë¶„ì„ ì‹œì‘ (ì´ {len(text)}ì)")
        chunks = self.text_splitter.split_text(text)

        all_entities = set()
        for i, chunk in enumerate(chunks):
            # (ë¡œê·¸ ë„ˆë¬´ ë§ìœ¼ë©´ ì¤„ì—¬ë„ ë¨)
            entities = self._extract_entities_from_text(chunk)
            all_entities.update(entities)

        known_settings = []
        unknown_terms = []

        for entity in all_entities:
            if entity in self.setting_keywords or any(k in entity for k in self.setting_keywords):
                known_settings.append(entity)
            else:
                unknown_terms.append(entity)

        history_context = []
        if unknown_terms:
            print(f"ğŸ” ì—­ì‚¬ DB ì¡°íšŒ ì‹œë„: {unknown_terms}")
            for term in unknown_terms:
                search_result = self.repo.search(query_text=term, n_results=1)

                # ChromaDB ê²°ê³¼ ë¶„í•´
                documents = search_result.get("documents", [[]])[0]
                distances = search_result.get("distances", [[]])[0]  # [ì¶”ê°€] ê±°ë¦¬ ì ìˆ˜ ê°€ì ¸ì˜¤ê¸°

                # [ì¤‘ìš”] ì„ê³„ê°’(Threshold) ì„¤ì •
                # ê±°ë¦¬ê°€ ê°€ê¹Œìš¸ìˆ˜ë¡ 0ì— ê°€ê¹ìŠµë‹ˆë‹¤. (L2 ê±°ë¦¬ ê¸°ì¤€)
                # ëª¨ë¸ë§ˆë‹¤ ë‹¤ë¥´ì§€ë§Œ, ë³´í†µ 1.2 ì´ìƒì´ë©´ "ë‹¤ë¥¸ ë‚´ìš©"ì¼ í™•ë¥ ì´ ë†’ìŠµë‹ˆë‹¤.
                DISTANCE_THRESHOLD = 1.2

                if documents and distances:
                    doc_content = documents[0]
                    dist = distances[0]

                    # ë””ë²„ê¹…ìš© ë¡œê·¸: ì‹¤ì œë¡œ ê±°ë¦¬ê°€ ì–¼ë§ˆ ë‚˜ì˜¤ëŠ”ì§€ í™•ì¸í•´ë³´ì„¸ìš”!
                    print(f"   ğŸ‘‰ '{term}' ê²€ìƒ‰ ê²°ê³¼ ê±°ë¦¬: {dist:.4f}")

                    if dist < DISTANCE_THRESHOLD:
                        history_context.append(f"[{term}]: {doc_content}")
                    else:
                        print(f"      âŒ ê±°ë¦¬ê°€ ë„ˆë¬´ ë©€ì–´ ì œì™¸ë¨ ({dist:.4f} >= {DISTANCE_THRESHOLD})")

        return {
            "found_entities_count": len(all_entities),
            "setting_terms_found": known_settings,
            "historical_terms_searched": unknown_terms,
            "retrieved_history_context": history_context
        }

    def _extract_entities_from_text(self, text: str) -> List[str]:
        # [ìˆ˜ì •] í•œêµ­ì–´ ë‰˜ì•™ìŠ¤ë¥¼ ë°˜ì˜í•˜ì—¬ êµ¬ì²´ì ì¸ ì œì™¸ ê·œì¹™ì„ ì„¤ì •
        prompt = """
        ë‹¹ì‹ ì€ ì—­ì‚¬ ì†Œì„¤ì˜ ê³ ì¦ì„ ë•ëŠ” ì „ë¬¸ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
        ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ 'ì—­ì‚¬ì  ë°°ê²½ ì§€ì‹'ì´ë‚˜ 'ë°±ê³¼ì‚¬ì „ ê²€ìƒ‰'ì´ í•„ìš”í•œ **ì¤‘ìš” í‚¤ì›Œë“œ(ê³ ìœ ëª…ì‚¬)**ë§Œ ì¶”ì¶œí•˜ì„¸ìš”.

        [ì¶”ì¶œ ê·œì¹™]
        1. **ëŒ€ìƒ (í¬í•¨):** - ì‹¤ì¡´í–ˆë˜ ì—­ì‚¬ì  ì¸ë¬¼ (ì˜ˆ: ì¡°ì§€í”„ ë¦¬ìŠ¤í„°, ê¹€ëŒ€ê±´ ì‹ ë¶€)
           - êµ¬ì²´ì ì¸ ì§€ëª…ì´ë‚˜ ê¸°ê´€ëª… (ì˜ˆ: ìœ ë‹ˆë²„ì‹œí‹° ì¹¼ë¦¬ì§€ ëŸ°ë˜, ë§ˆì¹´ì˜¤)
           - ì—­ì‚¬ì  ì‚¬ê±´, ìœ ë¬¼, ì¢…êµ/í•™ìˆ  ìš©ì–´ (ì˜ˆ: í€˜ì´ì»¤, ì„ë¯¸ì‚¬ë³€)

        2. **ì œì™¸ (ë¬´ì‹œ):** - í”í•œ ì¼ë°˜ ëª…ì‚¬ (ì˜ˆ: ì˜ê³¼ ëŒ€í•™, ë³‘ì›, ì˜êµ­ ì˜ì‚¬, ì´Œë†ˆ, ì§‘ì•ˆ)
           - ë‹¨ìˆœí•œ ì‹œê°„/ì¥ì†Œ í‘œí˜„ (ì˜ˆ: 21ì„¸ê¸°, í•œêµ­, ì˜êµ­, ì˜¤ëŠ˜ë‚ , ì•„ì¹¨)
           - ì†Œì„¤ ì† í—ˆêµ¬ì˜ ì¸ë¬¼ ì´ë¦„ì´ë‚˜ í˜¸ì¹­ (ìœ ëª… ìœ„ì¸ì´ ì•„ë‹ˆë©´ ì œì™¸)
             (ì˜ˆ: ì¸ì„ì´, ë†ˆ, ìë„¤)

        [ì¶œë ¥ í˜•ì‹]
        - ê²°ê³¼ëŠ” ì˜¤ì§ JSON ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œë§Œ ë°˜í™˜í•˜ì„¸ìš”.
        - ë¶€ì—° ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”.

        [ì˜ˆì‹œ]
        ì…ë ¥: "ì¸ì„ì´ëŠ” 19ì„¸ê¸° ëŸ°ë˜ì˜ ìœ ë‹ˆë²„ì‹œí‹° ì¹¼ë¦¬ì§€ ë³‘ì›ì—ì„œ ì¡°ì§€í”„ ë¦¬ìŠ¤í„°ë¥¼ ë§Œë‚¬ë‹¤."
        ì¶œë ¥: ["19ì„¸ê¸° ëŸ°ë˜", "ìœ ë‹ˆë²„ì‹œí‹° ì¹¼ë¦¬ì§€ ë³‘ì›", "ì¡°ì§€í”„ ë¦¬ìŠ¤í„°"]
        """

        try:
            response = self.llm.invoke([
                SystemMessage(content=prompt),
                HumanMessage(content=text[:3000])
            ])
            # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±° í›„ íŒŒì‹±
            content = response.content.replace("```json", "").replace("```", "").strip()
            result = json.loads(content)
            return result if isinstance(result, list) else []
        except Exception as e:
            print(f"âš ï¸ ì—”í‹°í‹° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []